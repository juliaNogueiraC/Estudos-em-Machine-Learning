{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKqbXKohH5taz7GZLHLXTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliaNogueiraC/Estudos-em-Machine-Learning/blob/main/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Neurais\n"
      ],
      "metadata": {
        "id": "AAwLH6JTGbxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron\n",
        "O perceptron é a unidade básica de uma rede neural. É um modelo simples de uma rede neural com uma única camada, usado para tarefas de classificação binária."
      ],
      "metadata": {
        "id": "QsWMsm-0GgVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "t2EcG8B6GVoN",
        "outputId": "31451f2a-2c1e-4211-a98e-e08e8adfbb18"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of informative, redundant and repeated features must sum to less than the number of total features",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-25d91c7b61cc>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Criar um conjunto de dados de exemplo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Dividir os dados em conjunto de treinamento e teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_samples_generator.py\u001b[0m in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# Count features, clusters and samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_informative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_redundant\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_repeated\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;34m\"Number of informative, redundant and repeated \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;34m\"features must sum to less than the number of total\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of informative, redundant and repeated features must sum to less than the number of total features"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Criar um conjunto de dados de exemplo\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=0)\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Criar e treinar o Perceptron\n",
        "model = Perceptron()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões e avaliar o modelo\n",
        "y_pred = model.predict(X_test)\n",
        "print('Acurácia:', accuracy_score(y_test, y_pred))\n",
        "print('Relatório de Classificação:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redes Feedforward\n",
        "Redes neurais feedforward são compostas por camadas de neurônios onde as conexões são direcionadas e não cíclicas. Elas são usadas para tarefas de classificação e regressão."
      ],
      "metadata": {
        "id": "3JjHhcq9GkRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Carregar e preparar o conjunto de dados MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalização\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Construir o modelo\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('\\nAcurácia de Teste:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJbxrs6uGl6x",
        "outputId": "5da68076-e8a7-46af-97c6-aa02f778c377"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2880 - accuracy: 0.9181 - val_loss: 0.1595 - val_accuracy: 0.9548\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1252 - accuracy: 0.9636 - val_loss: 0.1172 - val_accuracy: 0.9650\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0843 - accuracy: 0.9748 - val_loss: 0.1049 - val_accuracy: 0.9698\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0636 - accuracy: 0.9811 - val_loss: 0.1007 - val_accuracy: 0.9707\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0480 - accuracy: 0.9854 - val_loss: 0.0912 - val_accuracy: 0.9733\n",
            "313/313 - 1s - loss: 0.0841 - accuracy: 0.9752 - 1s/epoch - 4ms/step\n",
            "\n",
            "Acurácia de Teste: 0.9751999974250793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "O backpropagation é um algoritmo usado para treinar redes neurais, ajustando os pesos das conexões com base no erro calculado na saída."
      ],
      "metadata": {
        "id": "77VRbuHCGs46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Redes Neurais Convolucionais (CNNs)\n",
        "CNNs são usadas principalmente para processamento de imagens e têm camadas convolucionais que extraem características dos dados."
      ],
      "metadata": {
        "id": "JFmCou2HGyBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Carregar e preparar o conjunto de dados CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalização\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Construir o modelo CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('\\nAcurácia de Teste:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-AgLIeDG07R",
        "outputId": "5f301887-9dd4-4a73-c6c1-93ce04fbd9db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 10s 0us/step\n",
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 60s 47ms/step - loss: 1.5582 - accuracy: 0.4273 - val_loss: 1.3121 - val_accuracy: 0.5323\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 1.1913 - accuracy: 0.5770 - val_loss: 1.1121 - val_accuracy: 0.6051\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 67s 54ms/step - loss: 1.0391 - accuracy: 0.6322 - val_loss: 1.0104 - val_accuracy: 0.6468\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.9371 - accuracy: 0.6702 - val_loss: 1.0078 - val_accuracy: 0.6439\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8600 - accuracy: 0.6992 - val_loss: 0.9472 - val_accuracy: 0.6722\n",
            "313/313 - 4s - loss: 0.9508 - accuracy: 0.6711 - 4s/epoch - 14ms/step\n",
            "\n",
            "Acurácia de Teste: 0.6711000204086304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "edes Neurais Convolucionais (CNNs) são uma classe poderosa de redes neurais projetadas para processar dados com estrutura de grade, como imagens. Elas utilizam camadas convolucionais para capturar características espaciais e temporais dos dados. Vou explicar os principais componentes e fornecer exemplos práticos de código para ilustrar como as CNNs funcionam.\n",
        "\n",
        "Componentes das Redes Neurais Convolucionais\n",
        "Camadas Convolucionais:\n",
        "\n",
        "Aplicam filtros (ou kernels) para extrair características locais das imagens.\n",
        "Cada filtro é treinado para detectar um padrão específico (como bordas, texturas, etc.).\n",
        "Camadas de Pooling (Subsampling):\n",
        "\n",
        "Reduzem a dimensão espacial das características extraídas, mantendo as informações mais importantes.\n",
        "Tipicamente usam operações de max pooling ou average pooling.\n",
        "Camadas de Flatten:\n",
        "\n",
        "Transformam a matriz de características em um vetor unidimensional, preparando-a para a camada densa final.\n",
        "Camadas Densas (Fully Connected):\n",
        "\n",
        "Conectam todos os neurônios da camada anterior a todos os neurônios da camada atual.\n",
        "Usadas para a classificação final ou regressão.\n",
        "Exemplo Prático com CNNs\n",
        "Vamos usar o conjunto de dados MNIST, que contém imagens de dígitos manuscritos, para construir e treinar uma CNN."
      ],
      "metadata": {
        "id": "95eD7WGcHjR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Carregar e preparar o conjunto de dados MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalização\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)  # Adicionar dimensão de canal\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Construir o modelo CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('\\nAcurácia de Teste:', test_acc)\n",
        "\n",
        "# Plotar o histórico de treinamento\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Acurácia de Treinamento')\n",
        "plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Vw5eF22Hk7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicação do Código:\n",
        "\n",
        "Carregamento e Preparação dos Dados:\n",
        "\n",
        "O conjunto de dados MNIST é carregado e normalizado.\n",
        "As imagens são redimensionadas para ter uma dimensão de canal (1 para imagens em escala de cinza).\n",
        "As etiquetas são convertidas em vetores de classe usando to_categorical.\n",
        "\n",
        "Construção do Modelo CNN:\n",
        "\n",
        "Conv2D: Camadas convolucionais com 32, 64 e 64 filtros, respectivamente. Cada filtro é de tamanho 3x3.\n",
        "MaxPooling2D: Camadas de pooling para reduzir a dimensão espacial das características extraídas.\n",
        "Flatten: Converte a saída da camada convolucional em um vetor unidimensional.\n",
        "Dense: Camadas densas para a classificação final. A última camada tem 10 neurônios, correspondendo às 10 classes (dígitos 0-9).\n",
        "\n",
        "Compilação e Treinamento:\n",
        "\n",
        "O modelo é compilado com o otimizador Adam e a função de perda categorical_crossentropy.\n",
        "O modelo é treinado por 5 épocas com uma divisão de validação de 20%.\n",
        "\n",
        "Avaliação e Visualização:\n",
        "\n",
        "O modelo é avaliado no conjunto de teste.\n",
        "O histórico de treinamento é plotado para visualizar a acurácia de treinamento e validação ao longo das épocas.\n",
        "Experimentos Adicionais:\n",
        "Ajuste de Hiperparâmetros: Experimente diferentes números de filtros, tamanhos de kernel e camadas.\n",
        "Aumento de Dados: Use técnicas de aumento de dados (data augmentation) para melhorar a robustez do modelo.\n",
        "Modelos Pré-Treinados: Explore redes pré-treinadas como VGG, ResNet e Inception para tarefas mais complexas."
      ],
      "metadata": {
        "id": "LqcCs01qHpWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Neurais Recorrentes (RNNs)\n",
        "RNNs são usadas para dados sequenciais, como séries temporais e NLP. Elas possuem conexões recorrentes que permitem que a informação persista.\n",
        "\n",
        "Exemplo de RNN para Séries Temporais com Keras:"
      ],
      "metadata": {
        "id": "cPvXVmZPG3wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Gerar dados de exemplo\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 10, 1)  # 100 amostras, 10 passos de tempo, 1 característica\n",
        "y = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "# Dividir os dados em treinamento e teste\n",
        "X_train, X_test = X[:80], X[80:]\n",
        "y_train, y_test = y[:80], y[80:]\n",
        "\n",
        "# Construir o modelo RNN\n",
        "model = Sequential([\n",
        "    SimpleRNN(50, input_shape=(10, 1), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('\\nAcurácia de Teste:', test_acc)\n"
      ],
      "metadata": {
        "id": "0gwrdr3XG58i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Generativas Adversariais (GANs)\n",
        "GANs são usadas para gerar dados novos e realistas, como imagens, a partir de dados existentes. Consistem em duas redes: o gerador e o discriminador.\n",
        "\n",
        "Exemplo Básico de GAN com TensorFlow/Keras:\n",
        "\n",
        "Este exemplo é mais avançado e geralmente requer mais código, então aqui está um exemplo simplificado:"
      ],
      "metadata": {
        "id": "nA69nTDsG82B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Gerador\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(100,)),\n",
        "        Dense(784, activation='sigmoid'),\n",
        "        Reshape((28, 28, 1))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminador\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28, 1)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Criar GAN\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    model = tf.keras.Model(gan_input, gan_output)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "    return model\n",
        "\n",
        "# Instanciar modelos\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Treinamento (simplificado)\n",
        "def train_gan(gan, generator, discriminator, epochs=1000, batch_size=128):\n",
        "    for epoch in range(epochs):\n",
        "        # Gerar dados falsos\n",
        "        noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
        "        generated_images = generator.predict(noise)\n",
        "        # Treinar o discriminador\n",
        "        real_images = np.random.rand(batch_size, 28, 28, 1)  # Imagens reais de exemplo\n",
        "        X = np.concatenate([real_images, generated_images])\n",
        "        y_dis = np.zeros(2 * batch_size)\n",
        "        y_dis[:batch_size] = 0.9  # Rótulo de verdadeiros\n",
        "        discriminator.trainable = True\n",
        "        d_loss = discriminator.train_on_batch(X, y_dis)\n",
        "        # Treinar o gerador\n",
        "        noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
        "        y_gen = np.ones(batch_size)\n",
        "        discriminator.trainable = False\n",
        "        g_loss = gan.train_on_batch(noise, y_gen)\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch} | D Loss: {d_loss} | G Loss: {g_loss}')\n",
        "\n",
        "# Treinar a GAN\n",
        "train_gan(gan, generator, discriminator)\n"
      ],
      "metadata": {
        "id": "kn3FRRn-G-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicação das Partes da GAN:\n",
        "\n",
        "Gerador:\n",
        "\n",
        "Objetivo: Criar novas amostras (como imagens) que sejam semelhantes às amostras reais do conjunto de dados.\n",
        "Estrutura: Geralmente começa com uma camada densa para transformar o vetor de entrada (geralmente um vetor de ruído) em uma forma de imagem e, em seguida, utiliza camadas adicionais para refinar essa forma até que se assemelhe a uma imagem real.\n",
        "Discriminador:\n",
        "\n",
        "Objetivo: Diferenciar entre as imagens reais do conjunto de dados e as imagens geradas pelo gerador.\n",
        "Estrutura: Recebe uma imagem e passa por camadas densas que ajudam a classificar se a imagem é real ou falsa (gerada).\n",
        "GAN (Generative Adversarial Network):\n",
        "\n",
        "Objetivo: Treinar o gerador para criar imagens que o discriminador não consiga distinguir das imagens reais.\n",
        "Estrutura: Combina o gerador e o discriminador em uma rede. O gerador tenta enganar o discriminador, enquanto o discriminador tenta se tornar melhor em distinguir entre imagens reais e falsas.\n",
        "Treinamento da GAN:\n",
        "\n",
        "Treinamento do Discriminador:\n",
        "\n",
        "Passo 1: O discriminador é treinado com um conjunto de imagens reais e um conjunto de imagens geradas pelo gerador.\n",
        "Passo 2: As imagens reais são rotuladas como verdadeiras e as imagens geradas como falsas. O discriminador tenta melhorar sua capacidade de distinguir entre essas duas categorias.\n",
        "Treinamento do Gerador:\n",
        "\n",
        "Passo 1: O gerador cria novas imagens a partir de um vetor de ruído.\n",
        "Passo 2: Essas imagens geradas são passadas ao discriminador. O objetivo do gerador é enganar o discriminador, fazendo com que ele classifique as imagens geradas como reais.\n",
        "Passo 3: O gerador é treinado com base na perda do discriminador. Se o discriminador considera as imagens geradas como reais, o gerador está sendo bem-sucedido.\n",
        "O treinamento continua alternando entre essas duas etapas, aprimorando tanto o gerador quanto o discriminador ao longo do tempo.\n",
        "\n",
        "Resumo das Principais Técnicas de Deep Learning:\n",
        "\n",
        "Perceptron: Unidade básica de redes neurais, usada para classificação binária.\n",
        "Redes Feedforward: Redes neurais com camadas direcionadas, usadas para tarefas de classificação e regressão.\n",
        "Backpropagation: Algoritmo para ajustar os pesos da rede neural durante o treinamento.\n",
        "Redes Neurais Convolucionais (CNNs): Utilizadas para processamento de imagens, aplicando operações convolucionais.\n",
        "Redes Neurais Recorrentes (RNNs): Usadas para dados sequenciais, como séries temporais e processamento de linguagem natural (NLP).\n",
        "Redes Generativas Adversariais (GANs): Usadas para gerar dados novos e realistas, como imagens, a partir de dados existentes.\n",
        "Frameworks de Deep Learning:\n",
        "\n",
        "TensorFlow: Framework poderoso desenvolvido pelo Google, ideal para construir e treinar modelos complexos de deep learning.\n",
        "Keras: API de alto nível para TensorFlow, simplifica a construção e o treinamento de redes neurais.\n",
        "PyTorch: Framework desenvolvido pelo Facebook, conhecido por sua flexibilidade e facilidade de uso, especialmente para pesquisa e desenvolvimento."
      ],
      "metadata": {
        "id": "9pZ4hHdNHO6F"
      }
    }
  ]
}